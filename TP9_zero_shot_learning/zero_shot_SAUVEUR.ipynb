{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP ZERO-SHOT CLASSIFICATION\n",
    "\n",
    "### Réalisé par Mathieu SAUVEUR \n",
    "\n",
    "**Note avant de commencer : quelques parties de ce TP peuvent être légèrement similaires à celles de Byong Hee Lee car nous avons travaillé ensemble, cependant nous remettons chacun notre propre travail.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_But du TP_ :** *Le but de ce TP est de reproduire une pipeline de zero-shot classification sur deux datasets avec deux scoring function, suivant l'article suivant: Small language models for Zero-shot classification. Nous avons pu nous baser du tutoriel Hugging Face : CS224N: Hugging Face Transformers Tutorial (Spring '24). Nous avons utilisé comme modèle initial TinyStories (1 million de paramètres) ainsi que quelques une de ses versions un peu larges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premier dataset : **ag_news** \n",
    "\n",
    "ag_news est un dataset de classification de texte qui contient des articles de presse provenant de 4 catégories différentes : World, Sports, Business et Science/Technology. \n",
    "\n",
    "### Les scoring funcions utilisées pour l'évaluation des modèles sont :\n",
    "- **Probabilities** : La meilleure prédiction est celle qui a la probabilité la plus élevée.\n",
    "- **DCPMI** : La meilleure prédiction est celle qui a le score DCPMI le plus élevé. Le score DCPMI étant le ration entre la probabilité d'un certain label par rapport à un texte donné et la probabilité de ce même label par rapport à un texte générique généré spécifiquement pour ce label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(dataset['train']['label'])\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at roneneldan/TinyStories-1M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"roneneldan/TinyStories-1M\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"roneneldan/TinyStories-33M\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"calum/tinystories-gpt2-3M\")\n",
    "\n",
    "labels = [\"World\", \"Sports\", \"Business\", \"Science/Technology\"] # Labels associés récupérés sur Kaggle/Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(probabilities, conditional_probabilities=None, scoring_function=\"Probability\"):\n",
    "    \"\"\"_summary_ \n",
    "        Calculate the score of the predicted label based on the probabilities of the labels.\n",
    "        If scoring_function is \"Probability\", the score is the maximum probability.\n",
    "        If scoring_function is \"DCPMI\", the score is the maximum DCPMI.\n",
    "\n",
    "    Args:\n",
    "        probabilities (list): The probabilities of each label.\n",
    "        conditional_probabilities (list): The conditional probabilities of each label.\n",
    "        scoring_function (str): The scoring function to use.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the predicted label.\n",
    "    \"\"\"\n",
    "    if scoring_function == \"Probability\":\n",
    "        return probabilities.index(max(probabilities))\n",
    "    elif scoring_function == \"DCPMI\":\n",
    "        dcpmis = [prob / conditional_probabilities[i][i] for i, prob in enumerate(probabilities)]\n",
    "        return dcpmis.index(max(dcpmis))\n",
    "    # On pourrait rajouter ici d'autre elif pour des fonctions de scoring différentes\n",
    "\n",
    "\n",
    "def get_conditional_probabilities(labels, classifier):\n",
    "    \"\"\"_summary_\n",
    "        Get the conditional probabilities for each label.\n",
    "\n",
    "    Args:\n",
    "        labels (list): The list of labels.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of conditional probabilities.\n",
    "    \"\"\"\n",
    "    conditional_probabilities = []\n",
    "    for label in labels:\n",
    "        input_text = f\"This article discusses topics related to {label}.\"\n",
    "        result = classifier(input_text, candidate_labels=labels)\n",
    "        label_to_prob = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        ordered_probs = [label_to_prob[label] for label in labels]\n",
    "        conditional_probabilities.append(ordered_probs)\n",
    "        print(f\"Text about {label}  --> Conditional probabilities for it being either World/Sports/Business/Science&Technology: {ordered_probs}\") # On affiche ici le résultat pour chaque phrase témoin relative à chaque label\n",
    "    return conditional_probabilities\n",
    "\n",
    "\n",
    "def apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"Probability\"):\n",
    "    \"\"\"_summary_\n",
    "        Apply the model to the dataset and calculate the accuracy of the predictions.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to evaluate.\n",
    "        labels (list): The list of labels.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "        scoring_function (str): The scoring function to use.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "        DataFrame: The comparison between the true and predicted labels.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # On ne calcul les probabilités conditionnelles que si on utilise la DCPMI\n",
    "    if scoring_function==\"DCPMI\":\n",
    "        conditional_probabilities = get_conditional_probabilities(labels, classifier)\n",
    "    n=0\n",
    "    nb_items_processed = True       # Paramètre, si initialisé sur False, permet d'afficher le résultat du traitement des n premiers items, comprenant le texte, les labels, les probabilités et le label prédit\n",
    "\n",
    "    for item in tqdm(dataset['test'].select(range(3000)), desc=\"Processing\"):       # Ici on va sélectionner le nombre d'élement sur lesquels on veut tester le modèle. pour tester sur tous les élements, on supprime .select()\n",
    "        text = item['text'] \n",
    "        true_label = item['label']\n",
    "        y_true.append(true_label)\n",
    "        \n",
    "        # On initialise le prompt donné au classifieur\n",
    "        prompt = f\"Is the following sentence negative, positive or neutral?\\n{text}\"            \n",
    "        # On le lui donne avec les labels candidats\n",
    "        result = classifier(prompt, candidate_labels=labels)         \n",
    "\n",
    "        # Les probabilité obtenues sont classées de la plus grande à la plus faible dans le résultat, on va donc les remettre dans l'ordre des labels :\n",
    "        label_to_prob = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        probabilities = [label_to_prob[label] for label in labels]\n",
    "\n",
    "        # On fait les prédictions en fonction de la fonction de scoring choisie\n",
    "        if scoring_function==\"Probability\":\n",
    "            predicted_label = calculate_score(probabilities, scoring_function)\n",
    "            y_pred.append(predicted_label)\n",
    "            \n",
    "        elif scoring_function==\"DCPMI\":\n",
    "            predicted_label = calculate_score(probabilities, conditional_probabilities, scoring_function)\n",
    "            y_pred.append(predicted_label)\n",
    "\n",
    "        if not nb_items_processed: # Boucle qui affiche les n premiers items traités\n",
    "            print(f\"\\nItem n°{n+1}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Labels: {labels}\")\n",
    "            print(f\"Probabilities: {probabilities}\")\n",
    "            print(f\"Predicted label: {predicted_label}\\n\")\n",
    "            n += 1\n",
    "            if n == 1:\n",
    "                three_item_processed = True\n",
    "\n",
    "    # On va calculer l'accuracy de notre prédiction zero-shot pour tous les items étudiés\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # On créé un DataFrame pour comparer les labels prédits et les labels réels, qui va être retourné en même temps que l'accuracy\n",
    "    comparaison_true_predicted = pd.DataFrame({\"True Label\": y_true, \"Predicted Label\": y_pred})\n",
    "\n",
    "    return accuracy, comparaison_true_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des prédictions zero-shot sur le dataframe (modèle basique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3000/3000 [15:23<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : 0.26166666666666666  <<<<<<<\n",
      "\n",
      "Text about World  --> Conditional probabilities for it being either World/Sports/Business/Science/Technology: [0.22768542170524597, 0.2462114691734314, 0.24311888217926025, 0.28298428654670715]\n",
      "Text about Sports  --> Conditional probabilities for it being either World/Sports/Business/Science/Technology: [0.22698958218097687, 0.24694472551345825, 0.24610139429569244, 0.27996426820755005]\n",
      "Text about Business  --> Conditional probabilities for it being either World/Sports/Business/Science/Technology: [0.22893641889095306, 0.24610473215579987, 0.2467629760503769, 0.2781957685947418]\n",
      "Text about Science/Technology  --> Conditional probabilities for it being either World/Sports/Business/Science/Technology: [0.23483063280582428, 0.2503969073295593, 0.24123473465442657, 0.2735377550125122]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3000/3000 [12:57<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : 0.24766666666666667  <<<<<<<\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# On appelle la fonction apply_model_and_evaluate avec la scoring_function \"Probability\" (implémentée de base)\n",
    "accuracy_proba, comp_proba = apply_model_and_evaluate(dataset, labels, classifier)\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : {accuracy_proba}  <<<<<<<\\n\")\n",
    "\n",
    "# On appelle la fonction apply_model_and_evaluate avec la scoring_function \"DCPMI\"\n",
    "accuracy_DCPMI, comp_DCPMI = apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"DCPMI\")\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : {accuracy_DCPMI}  <<<<<<<\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des prédictions zero-shot sur le dataframe (modèle plus large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at calum/tinystories-gpt2-3M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Processing:   0%|          | 0/3000 [00:00<?, ?it/s]Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n",
      "Processing: 100%|██████████| 3000/3000 [08:24<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : 0.25833333333333336  <<<<<<<\n",
      "\n",
      "Text about no  --> Conditional probabilities for it being either World/Sports/Business/Science&Technology: [0.43308448791503906, 0.5669155716896057]\n",
      "Text about yes  --> Conditional probabilities for it being either World/Sports/Business/Science&Technology: [0.4498637616634369, 0.5501362681388855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3000/3000 [08:31<00:00,  5.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : 0.24833333333333332  <<<<<<<\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"calum/tinystories-gpt2-3M\")\n",
    "\n",
    "# On appelle la fonction apply_model_and_evaluate avec la scoring_function \"Probability\" (implémentée de base)\n",
    "accuracy_proba, comp_proba = apply_model_and_evaluate(dataset, labels, classifier)\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : {accuracy_proba}  <<<<<<<\\n\")\n",
    "\n",
    "# On appelle la fonction apply_model_and_evaluate avec la scoring_function \"DCPMI\"\n",
    "accuracy_DCPMI, comp_DCPMI = apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"DCPMI\")\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : {accuracy_DCPMI}  <<<<<<<\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture du code utilisé dans les parties suivantes est la même que celle présentée dans cette partie. C'est pourquoi, mis à part les docstrings, les fonctions ne seront pas commentées dans les parties suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième dataset : **financial_phrasebank** \n",
    "\n",
    "financial_phrasebank est un dataset de classification de texte qui contient des phrases financières provenant de 3 catégories différentes : neutral, positive et negative.\n",
    "\n",
    "### Les scoring funcions utilisées pour l'évaluation des modèles sont :\n",
    "- **Probabilities** : La meilleure prédiction est celle qui a la probabilité la plus élevée.\n",
    "- **DCPMI** : La meilleure prédiction est celle qui a le score DCPMI le plus élevé. Le score DCPMI étant le ration entre la probabilité d'un certain label par rapport à un texte donné et la probabilité de ce même label par rapport à un texte générique généré spécifiquement pour ce label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for financial_phrasebank contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/financial_phrasebank\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 2264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(dataset['train']['label'])\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at roneneldan/TinyStories-1M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"roneneldan/TinyStories-1M\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"roneneldan/TinyStories-33M\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"calum/tinystories-gpt2-3M\")\n",
    "\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(probabilities, conditional_probabilities=None, scoring_function=\"Probability\"):\n",
    "    \"\"\"_summary_ \n",
    "        Calculate the score of the predicted label based on the probabilities of the labels.\n",
    "        If scoring_function is \"Probability\", the score is the maximum probability.\n",
    "        If scoring_function is \"DCPMI\", the score is the maximum DCPMI.\n",
    "\n",
    "    Args:\n",
    "        probabilities (list): The probabilities of each label.\n",
    "        conditional_probabilities (list): The conditional probabilities of each label.\n",
    "        scoring_function (str): The scoring function to use.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the predicted label.\n",
    "    \"\"\"\n",
    "    if scoring_function == \"Probability\":\n",
    "        return probabilities.index(max(probabilities))\n",
    "    elif scoring_function == \"DCPMI\":\n",
    "        dcpmis = [prob / conditional_probabilities[i][i] for i, prob in enumerate(probabilities)]\n",
    "        return dcpmis.index(max(dcpmis))\n",
    "    return 0\n",
    "\n",
    "def get_conditional_probabilities(labels, classifier):\n",
    "    \"\"\"_summary_\n",
    "        Get the conditional probabilities for each label.\n",
    "\n",
    "    Args:\n",
    "        labels (list): The list of labels.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of conditional probabilities.\n",
    "    \"\"\"\n",
    "    conditional_probabilities = []\n",
    "    for label in labels:\n",
    "        input_text = f\"This sentence is {label}.\"\n",
    "        result = classifier(input_text, candidate_labels=labels)\n",
    "        label_to_prob = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        ordered_probs = [label_to_prob[label] for label in labels]\n",
    "        conditional_probabilities.append(ordered_probs)\n",
    "        print(f\"For the {label} sentence  --> Conditional probabilities for it being either negative/neutral/positive: {ordered_probs}\")\n",
    "    return conditional_probabilities\n",
    "\n",
    "def apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"Probability\"):\n",
    "    \"\"\"_summary_\n",
    "        Apply the model to the dataset and calculate the accuracy of the predictions.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to evaluate.\n",
    "        labels (list): The list of labels.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "        scoring_function (str): The scoring function to use.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "        DataFrame: The comparison between the true and predicted labels.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    if scoring_function==\"DCPMI\":\n",
    "        conditional_probabilities = get_conditional_probabilities(labels, classifier)\n",
    "    n=0\n",
    "    nb_items_processed = True \n",
    "\n",
    "    for item in tqdm(dataset['train'], desc=\"Processing\"):  # .select(range(500))\n",
    "        text = item['sentence'] \n",
    "        true_label = item['label']\n",
    "        y_true.append(true_label)\n",
    "        \n",
    "        prompt = f\"Is the following sentence negative, positive or neutral?\\n{text}\" #\n",
    "        result = classifier(prompt, candidate_labels=labels)\n",
    "        label_to_prob = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        probabilities = [label_to_prob[label] for label in labels]\n",
    "\n",
    "        if scoring_function==\"Probability\":\n",
    "            predicted_label = calculate_score(probabilities, scoring_function)\n",
    "            y_pred.append(predicted_label)\n",
    "            \n",
    "        elif scoring_function==\"DCPMI\":\n",
    "            predicted_label = calculate_score(probabilities, conditional_probabilities, scoring_function)\n",
    "            y_pred.append(predicted_label)\n",
    "\n",
    "        if not nb_items_processed: \n",
    "            print(f\"\\nItem n°{n+1}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Labels: {labels}\")\n",
    "            print(f\"Probabilities: {probabilities}\")\n",
    "            print(f\"Predicted label: {predicted_label}\\n\")\n",
    "            n += 1\n",
    "            if n == 1:\n",
    "                three_item_processed = True\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    comparaison_true_predicted = pd.DataFrame({\"True Label\": y_true, \"Predicted Label\": y_pred})\n",
    "\n",
    "    return accuracy, comparaison_true_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des prédictions zero-shot sur les dataframe (modèle basique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2264 [00:00<?, ?it/s]Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n",
      "Processing: 100%|██████████| 2264/2264 [05:36<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : 0.22924028268551236  <<<<<<<\n",
      "\n",
      "For the negative sentence  --> Conditional probabilities for it being either negative/neutral/positive: [0.34229576587677, 0.31504562497138977, 0.3426586389541626]\n",
      "For the neutral sentence  --> Conditional probabilities for it being either negative/neutral/positive: [0.3448764681816101, 0.3156158924102783, 0.3395076394081116]\n",
      "For the positive sentence  --> Conditional probabilities for it being either negative/neutral/positive: [0.3425242602825165, 0.3149176836013794, 0.3425580561161041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2264/2264 [06:07<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : 0.5552120141342756  <<<<<<<\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_proba, comp_proba = apply_model_and_evaluate(dataset, labels, classifier)\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : {accuracy_proba}  <<<<<<<\\n\")\n",
    "\n",
    "accuracy_DCPMI, comp_DCPMI = apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"DCPMI\")\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : {accuracy_DCPMI}  <<<<<<<\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des prédictions zero-shot sur les dataframe (modèle plus large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at calum/tinystories-gpt2-3M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Processing:   0%|          | 0/2264 [00:00<?, ?it/s]Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n",
      "Processing: 100%|██████████| 2264/2264 [04:06<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : 0.15812720848056538  <<<<<<<\n",
      "\n",
      "For the no sentence  --> Conditional probabilities for it being either negative/neutral/positive: [0.5022147297859192, 0.4977853000164032]\n",
      "For the yes sentence  --> Conditional probabilities for it being either negative/neutral/positive: [0.5056576728820801, 0.49434226751327515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2264/2264 [04:10<00:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : 0.2628091872791519  <<<<<<<\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"calum/tinystories-gpt2-3M\")\n",
    "\n",
    "accuracy_proba, comp_proba = apply_model_and_evaluate(dataset, labels, classifier)\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : {accuracy_proba}  <<<<<<<\\n\")\n",
    "\n",
    "accuracy_DCPMI, comp_DCPMI = apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"DCPMI\")\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : {accuracy_DCPMI}  <<<<<<<\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troisième dataset : **sms_spam** \n",
    "\n",
    "sms_spam est un dataset de classification de texte qui contient des messages SMS provenant de 2 catégories différentes : spam sms et normal sms. Il y a donc deux label [no, yes] en réponse à la question \"Is this message spam ?\".\n",
    "\n",
    "### Les scoring funcions utilisées pour l'évaluation des modèles sont :\n",
    "- **Probabilities** : La meilleure prédiction est celle qui a la probabilité la plus élevée.\n",
    "- **DCPMI** : La meilleure prédiction est celle qui a le score DCPMI le plus élevé. Le score DCPMI étant le ration entre la probabilité d'un certain label par rapport à un texte donné et la probabilité de ce même label par rapport à un texte générique généré spécifiquement pour ce label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for ucirvine/sms_spam contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ucirvine/sms_spam\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 5574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ucirvine/sms_spam\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(dataset['train']['label'])\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at roneneldan/TinyStories-1M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"roneneldan/TinyStories-1M\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"roneneldan/TinyStories-33M\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"calum/tinystories-gpt2-3M\")\n",
    "\n",
    "labels = ['no', 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(probabilities, conditional_probabilities=None, scoring_function=\"Probability\"):\n",
    "    \"\"\"_summary_ \n",
    "        Calculate the score of the predicted label based on the probabilities of the labels.\n",
    "        If scoring_function is \"Probability\", the score is the maximum probability.\n",
    "        If scoring_function is \"DCPMI\", the score is the maximum DCPMI.\n",
    "\n",
    "    Args:\n",
    "        probabilities (list): The probabilities of each label.\n",
    "        conditional_probabilities (list): The conditional probabilities of each label.\n",
    "        scoring_function (str): The scoring function to use.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the predicted label.\n",
    "    \"\"\"\n",
    "    if scoring_function == \"Probability\":\n",
    "        return probabilities.index(max(probabilities))\n",
    "    elif scoring_function == \"DCPMI\":\n",
    "        dcpmis = [prob / conditional_probabilities[i][i] for i, prob in enumerate(probabilities)]\n",
    "        return dcpmis.index(max(dcpmis))\n",
    "    return 0\n",
    "\n",
    "def get_conditional_probabilities(classifier, domains =['a normal sms', 'a spam sms']):\n",
    "    \"\"\"_summary_\n",
    "        Get the conditional probabilities of each label for each domain.\n",
    "\n",
    "    Args:\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "        domains (list): The list of domains.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of conditional probabilities for each domain.\n",
    "    \"\"\"\n",
    "    conditional_probabilities = []\n",
    "    for domain in domains:\n",
    "        input_text = f\"This message is {domain}\"\n",
    "        result = classifier(input_text, candidate_labels=domains)\n",
    "        label_to_prob = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        ordered_probs = [label_to_prob[domain] for domain in domains]\n",
    "        conditional_probabilities.append(ordered_probs)\n",
    "        print(f\"For {domain} --> Conditional probabilities for it being a spam (normal/spam): {ordered_probs}\")\n",
    "    return conditional_probabilities\n",
    "\n",
    "def apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"Probability\"):\n",
    "    \"\"\"_summary_\n",
    "        Apply the model to the dataset and calculate the accuracy of the predictions.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to evaluate.\n",
    "        labels (list): The list of labels.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "        scoring_function (str): The scoring function to use.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "        DataFrame: The comparison between the true and predicted labels.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    if scoring_function==\"DCPMI\":\n",
    "        conditional_probabilities = get_conditional_probabilities(classifier)\n",
    "    n=0\n",
    "    nb_items_processed = True \n",
    "\n",
    "    for item in tqdm(dataset['train'].select(range(3000)), desc=\"Processing\"): \n",
    "        text = item['sms'] \n",
    "        true_label = item['label']\n",
    "        y_true.append(true_label)\n",
    "        \n",
    "        prompt = f\"Is the following message a spam? Answer by yes or no.\\n{text}\" \n",
    "        result = classifier(prompt, candidate_labels=labels)\n",
    "        label_to_prob = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        probabilities = [label_to_prob[label] for label in labels]\n",
    "\n",
    "        if scoring_function==\"Probability\":\n",
    "            predicted_label = calculate_score(probabilities, scoring_function)\n",
    "            y_pred.append(predicted_label)\n",
    "            \n",
    "        elif scoring_function==\"DCPMI\":\n",
    "            predicted_label = calculate_score(probabilities, conditional_probabilities, scoring_function)\n",
    "            y_pred.append(predicted_label)\n",
    "\n",
    "        if not nb_items_processed: \n",
    "            print(f\"\\nItem n°{n+1}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Labels: {labels}\")\n",
    "            print(f\"Probabilities: {probabilities}\")\n",
    "            print(f\"Predicted label: {predicted_label}\\n\")\n",
    "            n += 1\n",
    "            if n == 1:\n",
    "                three_item_processed = True\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    comparaison_true_predicted = pd.DataFrame({\"True Label\": y_true, \"Predicted Label\": y_pred})\n",
    "\n",
    "    return accuracy, comparaison_true_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des prédictions zero-shot sur les dataframes (modèle basique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/3000 [00:00<?, ?it/s]Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n",
      "Processing: 100%|██████████| 3000/3000 [05:51<00:00,  8.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : 0.8406666666666667  <<<<<<<\n",
      "\n",
      "For a normal sms --> Conditional probabilities for it being a spam (normal/spam): [0.4985571503639221, 0.5014428496360779]\n",
      "For a spam sms --> Conditional probabilities for it being a spam (normal/spam): [0.49690547585487366, 0.503094494342804]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3000/3000 [05:42<00:00,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : 0.849  <<<<<<<\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_proba, comp_proba = apply_model_and_evaluate(dataset, labels, classifier)\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : {accuracy_proba}  <<<<<<<\\n\")\n",
    "\n",
    "accuracy_DCPMI, comp_DCPMI = apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"DCPMI\")\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : {accuracy_DCPMI}  <<<<<<<\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des prédictions zero-shot sur les dataframes (modèle plus large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at calum/tinystories-gpt2-3M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Processing:   0%|          | 0/3000 [00:00<?, ?it/s]Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n",
      "Processing: 100%|██████████| 3000/3000 [07:23<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : 0.8176666666666667  <<<<<<<\n",
      "\n",
      "For a normal sms --> Conditional probabilities for it being a spam (normal/spam): [0.4909922480583191, 0.5090077519416809]\n",
      "For a spam sms --> Conditional probabilities for it being a spam (normal/spam): [0.4946073889732361, 0.5053926706314087]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3000/3000 [06:15<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : 0.8506666666666667  <<<<<<<\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"calum/tinystories-gpt2-3M\")\n",
    "\n",
    "accuracy_proba, comp_proba = apply_model_and_evaluate(dataset, labels, classifier)\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using Probabilities is : {accuracy_proba}  <<<<<<<\\n\")\n",
    "\n",
    "accuracy_DCPMI, comp_DCPMI = apply_model_and_evaluate(dataset, labels, classifier, scoring_function=\"DCPMI\")\n",
    "print(f\">>>>>>>  The accuracy of the TinyStories model zero-shot prediction using DCPMI is : {accuracy_DCPMI}  <<<<<<<\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Étude des résultats obtenus pour les différents modèles:*\n",
    "\n",
    "1) Modèle TinyStories 1M (premier modèle)\n",
    "- En ce qui concerne le premier dataset on obtient des accuracy aux alentours de 25%/26% pour un nombre de labels possible de 4. Le modèle fait à peu près les mêmes prédictions qu'un modèle prédisant aléatoirement les labels (1 chance sur 4).\n",
    "Pour ce dataset, le meilleur modèle (MBZUAI/LaMini-GPT-124M) donne une accuracy de 73,4% donc, avec ce que j'ai testé nous en sommes loin.\n",
    "\n",
    "- Pour le deuxième dataset, j'ai 3 labels. J'obtiens 22% avec la fonction score 'Probabilité' et 55% avec la fonction score 'DPCMI'. La première fonction me donne moins bien que l'aléatoire (1 chance sur 3) tandisque la deuxième, un peu mieux (+20%)\n",
    "Pour ce dataset, les meilleur modèle (MBZUAI/LaMini-GPT-774M) donne un accuracy de 74,4%, donc avec DCPMI, nous nous en rapprochons légèrement.\n",
    "\n",
    "- Pour le troisième dataset, nous étions face à une tâche de spam classification, donc le label est binaire : soit le sms est un spam, soit il ne l'est pas. J'ai globalement des scores de l'ordre de 85% d'accuracy, ce qui est beaucoup. C'est d'ailleurs 15% de plus que le meilleur score obtenu par le modèle mosaicml/mpt-7b évoqué dans l'article. Cela me parait étrange, mais je n'ai pas trouvé l'endorit qui pourrait causerr cela dans mon code. \n",
    "Il est aussi bon de mentionner que lors de certaines exécutions du même code, il m'arrivait d'avoir des score de l'ordre de 16% uniquement.\n",
    "\n",
    "2) Modèle tinystories-gpt2-3MM (deuxième modèle)\n",
    "\n",
    "- Par rapport aux premiers résultats, pour le premier dataset, j'obtiens des accuracy équivalentes voire légèrement moins bonnes.\n",
    "\n",
    "- Par rapport aux premiers résultats, pour le second dataset, j'obtiens également des accuracy équivalentes voire légèrement moins bonnes.\n",
    "\n",
    "- Par rapport aux premiers résultats, pour le troisième dataset, j'obtiens les mêmes résultats.\n",
    "\n",
    "Les deux modèles testés ne sont peut-être pas suffisamment différents pour avec de meilleurs résultats. On peut cependant déduire que le modèle Tinystories n'est le mieux adapté pour éffectuer de la zero-shot classification. \n",
    "\n",
    "*Commentaire sur la scoring function DCPMI :*\n",
    "\n",
    "J'ai remarqué qu'une phrase témoin ayant vu un certain label n'est pratiquement jamais labélisé avec ce dit label, ce qui n'est pas normal. Pour que la fonction score DCPMI fonctionne à son plein potentiel, il serait plus pertinent que les labels les plus probabless pour des phrases témoins données soient les labels contenus dans ces dernières. J'ai essayé de résoudre ce souci, en vain. Et ce même en essayant avec des modèle beaucoup plus performants que ceux présenté dans ce nontebook. Cela n'empêche néanmoins pas que la fonction score DCPMI puisse fonctionner.\n",
    "\n",
    "*comparaison DCPMI et Probavilité*\n",
    "\n",
    "De manière générale, j'ai remarqué que les deux fonctions de scoring me permettaient d'avoir environ les mêmes résultats (mis à part pour le deuxième dataset dans lequel DCPMI était légèrement meilleur mais probablement dû à l'aléatoire favorable). Cependant, peut-être aurais-je pu obtenir dess résultats différents si ma fonction de scoring DCPMI avait mieux fonctionnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
